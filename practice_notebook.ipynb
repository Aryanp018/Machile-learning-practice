{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aryanp018/Machile-learning-practice/blob/main/practice_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp7_qcTkefin"
      },
      "source": [
        "### Practice Notebook for ML Research Poject\n",
        "This is a practice notebook for you to assess your own level of comfort with the ML framework **PyTorch**, which will play an important role during the research project.  \n",
        "If you haven't yet installed PyTorch, follow [the official installation instructions](https://pytorch.org/get-started/locally/). Make sure that you select the correct OS & select the version with CUDA if your computer supports it.\n",
        "If you do not have an Nvidia GPU, you can install the CPU version by setting `CUDA` to `None`. Note that some experience with the package is recommended for this notebook.  \n",
        "If you can successfully complete the tasks below, we think you are ready for the project.\n",
        "With that, good luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paHmpk6Sefir"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uy-L9TOMefis"
      },
      "source": [
        "### Task 1: Simple Custom Module: Dropout\n",
        "This task will test your ability to set up basic custom PyTorch modules. For this task, you are asked to re-implement the Dropout module. Dropout is a form of regularization for neural networks. It works by randomly setting activations (values) to 0, each one with equal probability `p`. The values are then scaled by a factor $\\frac{1}{1-p}$ to conserve their mean. Dropout effectively trains a pseudo-ensemble of models with stochastic gradient descent. During evaluation, we want to use the full ensemble and therefore have to turn off dropout. Use `self.training` to check if the model is in training or evaluation mode.  \n",
        "Do not use any dropout implementation from PyTorch for this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BC8-XQGmefis"
      },
      "outputs": [],
      "source": [
        "class Dropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Dropout, as described here:\n",
        "    https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
        "\n",
        "    Args:\n",
        "        p: float, dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, p: float):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, input: torch.Tensor):\n",
        "        \"\"\"\n",
        "        The module's forward pass.\n",
        "        `input` contains the activations to apply Dropout to.\n",
        "        Args:\n",
        "            input: PyTorch tensor, arbitrary shape\n",
        "\n",
        "        Returns:\n",
        "            PyTorch tensor, same shape as input\n",
        "        \"\"\"\n",
        "        # TODO: your implememation code\n",
        "        # the code is written below which mimics the pytorch's in-builts nn.dropout\n",
        "\n",
        "        if self.training: #it will only apply dropout in the training mode\n",
        "          mask = (torch.rand_like(input) > self.p).float() #this will generate the dropout mask\n",
        "          return (input * mask) / (1 - self.p) # Scalling the remainging activations\n",
        "        else:\n",
        "          return input #No dropout needed in evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mR_4sT9Defit",
        "outputId": "e536dc95-0d24-4152-eeb8-5b4297668655"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEST SUCCESSFULL\n"
          ]
        }
      ],
      "source": [
        "# Test dropout\n",
        "test = torch.rand(10_000)\n",
        "dropout = Dropout(0.2)\n",
        "test_dropped = dropout(test)\n",
        "\n",
        "# In principle, these assertions can fail due to bad luck, but\n",
        "# if implemented correctly they should almost always succeed.\n",
        "assert np.isclose(test_dropped.mean().item(), test.mean().item(), atol=1e-2)\n",
        "assert np.isclose((test_dropped > 0).float().mean().item(), 0.8, atol=1e-2)\n",
        "print(\"TEST SUCCESSFULL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuRhDMKLefit"
      },
      "source": [
        "**Question:** Why is it not necessary to implement a custom `backward` function for the `Dropout` module? Write your answer as a comment below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "i9RVFUctefit",
        "outputId": "03f6e185-e2c0-4396-d939-69b1eb8ab4d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n-> it is not necessary because of pytorch's autograd system which automatically computes gradients for each operations performed in the forward pass\\n\\n-> During backpropogation, the dropout mask is treated as a constant multiplier, meaning that the gradient of the input gets scaled by the same dropout mask.\\n\\n-> Explicit implementation is not needed because pytorch's computational graph always keeps the track of these operations and their gradients.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "'''\n",
        "-> it is not necessary because of pytorch's autograd system which automatically computes gradients for each operations performed in the forward pass\n",
        "\n",
        "-> During backpropogation, the dropout mask is treated as a constant multiplier, meaning that the gradient of the input gets scaled by the same dropout mask.\n",
        "\n",
        "-> Explicit implementation is not needed because pytorch's computational graph always keeps the track of these operations and their gradients.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx69gYLBefiu"
      },
      "source": [
        "### Task 2: Batch Normalization\n",
        "In this task, you are asked to re-implement classical Batch Normalization, which uses standardization instead of the kernel density-based method we propose in our project. It is defined as the function\n",
        "$ y = \\frac{x - \\mu_x}{\\sigma_x + \\epsilon} \\cdot \\gamma + \\beta $,\n",
        "where $\\gamma$ and $\\beta$ and learnable parameters and $\\epsilon$ is a some small number to avoid dividing by zero. The Statistics $\\mu_x$ and $\\sigma_x$ are taken separately for each feature. Since Batch Normalization comes up most often in Convolutional Neural Networks (CNNs), write the module so that it accepts image batches as input. For the normalization formula, this means averaging **over the batch and all pixels**. Use appropriae tensor slicing to achieve the computation of statistics along the correct dimensions. [This image](https://i.sstatic.net/DLwRc.png) may be helpful.\n",
        "\n",
        "Do not use any batch normalization implementation from PyTorch for this!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fu6oZkipefiu"
      },
      "outputs": [],
      "source": [
        "class BatchNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Batch normalization, similar to\n",
        "    https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d\n",
        "\n",
        "    Only uses batch statistics (no running mean for evaluation).\n",
        "    Batch statistics are calculated for a single dimension.\n",
        "    Gamma is initialized as 1, beta as 0.\n",
        "\n",
        "    Args:\n",
        "        num_features: Number of features to calculate batch statistics for.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_features):\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Initialize the required parameters\n",
        "        self.gamma = nn.Parameter(torch.ones(num_features))\n",
        "        self.beta = nn.Parameter(torch.zeros(num_features))\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        Batch normalization over the dimension C of (N, C, L).\n",
        "\n",
        "        Args:\n",
        "            input: PyTorch tensor, shape [N, C, L]\n",
        "\n",
        "        Return:\n",
        "            PyTorch tensor, same shape as input\n",
        "        \"\"\"\n",
        "        eps = 1e-5\n",
        "\n",
        "        N, C, *spatial_dims = input.shape\n",
        "\n",
        "        # TODO: Implement the required transformation\n",
        "        # the below is the required transformation as mentioned in the tasl\n",
        "\n",
        "        #this will allow us to calculate the statistics accross batch and all spatial dimentions\n",
        "\n",
        "        reshaped = input.transpose(0, 1).contiguous()\n",
        "        reshaped = reshaped.view(C, -1)\n",
        "\n",
        "        #Calculating the mean and cariance along the flattened batch and spatial dimentions\n",
        "        mean = reshaped.mean(dim = 1, keepdim = True) # [c,1]\n",
        "        var = reshaped.var(dim = 1, keepdim = True, unbiased = False) # [c,1]\n",
        "\n",
        "        #normalizing\n",
        "        reshaped_normalized = (reshaped - mean) / torch.sqrt(var + eps)\n",
        "\n",
        "        #reshaping back and applying the gamma and beta\n",
        "        output = reshaped_normalized.view(C, N, *spatial_dims)\n",
        "        output = output.transpose(0, 1).contiguous()\n",
        "\n",
        "        #adjusting the gamma and beta shapes for proper broadcasting across spatial dimensions\n",
        "        gamma_shape = [1, C] + [1] * len(spatial_dims)\n",
        "        beta_shape = [1, C] + [1] * len(spatial_dims)\n",
        "\n",
        "        return self.gamma.view(*gamma_shape) * output + self.beta.view(*beta_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZq5GGToefiu"
      },
      "outputs": [],
      "source": [
        "# Tests the batch normalization implementation\n",
        "torch.random.manual_seed(42)\n",
        "test = torch.randn(8, 2, 4)\n",
        "\n",
        "b1 = BatchNorm(2)\n",
        "test_b1 = b1(test)\n",
        "\n",
        "b2 = nn.BatchNorm1d(2, affine=False, track_running_stats=False)\n",
        "test_b2 = b2(test)\n",
        "\n",
        "assert torch.allclose(test_b1, test_b2, rtol=0.02)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jEV4SmUefiv"
      },
      "source": [
        "**Question:** Which feature is this implementation missing compared to PyTorch's `nn.BatchNorm1d` module? What is the advantage of having that feature? Write your answer as a comment below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7Af9N1xhefiv",
        "outputId": "253a003e-60ad-4281-e2ed-3551a00eae8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'-> Without running statistics, normalization depends solely on the current batch, making inference incosistent with varying batch sizes, impossible with single samplesand vulnerable to batch-specific\\n      variations - critical limitations for real world model deployment\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#This implementation lacks pytorch's nn.batchnorm1d feature of tracking running statistics, which is essential for inference.\n",
        "'''-> Without running statistics, normalization depends solely on the current batch, making inference incosistent with varying batch sizes, impossible with single samplesand vulnerable to batch-specific\n",
        "      variations - critical limitations for real world model deployment\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQiOCGT_efiv"
      },
      "source": [
        "### Task 3: Model Training\n",
        "In this task, you are asked to train a model which uses our kernel density normalization method. Be sure to follow good data science practices and report the model's performance in the end. Note that your task is not to create the model (you should use the `MLP_LN_LogReg` model we provide below), but to demonstrate your knowledge about the training process itself (dataset loading, optimization, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MES-tG3aefiv"
      },
      "outputs": [],
      "source": [
        "# The normalization module to be used with the Torch.NN model framework\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from typing import Type, Union\n",
        "from torch.nn import functional as F\n",
        "from math import prod\n",
        "ArrayLike = Union[torch.Tensor, np.ndarray, float]\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DensityKernel(ABC):\n",
        "    \"\"\"Abstract base class for kernel functions used in Kernel Density Estimation\"\"\"\n",
        "\n",
        "    @abstractmethod\n",
        "    def evaluate(x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Evaluate the kernel at all points in x\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def cdf(x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Evaluate the CDF of the kernel at all points in x\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def ppf(x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Evaluate the inverse CDF of the kernel at all points in x\"\"\"\n",
        "        pass\n",
        "\n",
        "class GaussianKernel(DensityKernel):\n",
        "    \"\"\"Gaussian kernel for density estimation\"\"\"\n",
        "\n",
        "    def evaluate(x: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.exp(-0.5 * torch.square(x)) / (np.sqrt(2 * np.pi))\n",
        "\n",
        "    def cdf(x: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * (1 + torch.erf(x / np.sqrt(2)))\n",
        "\n",
        "    def ppf(x: torch.Tensor) -> torch.Tensor:\n",
        "        return torch.erfinv(2*x - 1) * np.sqrt(2)\n",
        "\n",
        "class BandwidthHeuristic:\n",
        "    def scott(x: torch.Tensor) -> torch.Tensor:\n",
        "            return 1.059 * x.std() * x.numel() ** (-1/5)\n",
        "    def silverman(x: torch.Tensor) -> torch.Tensor:\n",
        "            return 0.9 * x.std() * x.numel() ** (-1/5)\n",
        "\n",
        "class KernelDensityEstimator:\n",
        "    \"\"\"Implements Kernel Density Estimation (KDE) with a given kernel\"\"\"\n",
        "\n",
        "    def __init__(self, data: torch.Tensor, kernel: Type[DensityKernel] = GaussianKernel, bandwidth: ArrayLike = None, bandwidth_heuristic: Type[BandwidthHeuristic] = BandwidthHeuristic.silverman):\n",
        "        self.data = data\n",
        "        self.kernel = kernel\n",
        "        if data is not None:\n",
        "            self.bandwidth = bandwidth if bandwidth is not None else bandwidth_heuristic(data)\n",
        "\n",
        "    def estimate(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Estimate the density at points x using the given kernel\"\"\"\n",
        "        scaled_x = (x[:,np.newaxis] - self.data[np.newaxis,:]) / self.bandwidth\n",
        "        return torch.mean(self.kernel.evaluate(scaled_x), dim=1) / self.bandwidth\n",
        "\n",
        "    def cdf(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Estimate the CDF at points x using the given kernel\"\"\"\n",
        "        scaled_x = (x[:,np.newaxis] - self.data[np.newaxis,:]) / self.bandwidth\n",
        "        return torch.mean(self.kernel.cdf(scaled_x), dim=1)\n",
        "\n",
        "    def normalize(self) -> torch.Tensor:\n",
        "        \"\"\"Normalize the data using the estimated CDF\"\"\"\n",
        "        cdf = self.cdf(self.data)\n",
        "        return self.kernel.ppf(cdf)\n",
        "\n",
        "    def normalize_data(self, data: torch.Tensor, bandwidth: ArrayLike = None, bandwidth_heuristic: Type[BandwidthHeuristic] = BandwidthHeuristic.silverman) -> torch.Tensor:\n",
        "        \"\"\"Normalize the given data using the estimated CDF\"\"\"\n",
        "        self.bandwidth = bandwidth if bandwidth is not None else bandwidth_heuristic(data)\n",
        "        self.data = data\n",
        "        cdf = self.cdf(data)\n",
        "        return self.kernel.ppf(cdf)\n",
        "\n",
        "class KDLayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Kernel Density Layer Normalization\\n\n",
        "    Applies Kernel Density Normalization over a miini-batch of inputs\n",
        "    Statistics are computed over the last `ndim` dimensions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 normalized_size: ArrayLike,\n",
        "                 bias: bool = True,\n",
        "                 kernel: Type[DensityKernel] = GaussianKernel,\n",
        "                 bandwidth: ArrayLike = None,\n",
        "                 bandwidth_heuristic: Type[BandwidthHeuristic] = BandwidthHeuristic.silverman,\n",
        "                 ):\n",
        "        super(KDLayerNorm, self).__init__()\n",
        "        self.kde = KernelDensityEstimator(None, kernel, bandwidth, bandwidth_heuristic)\n",
        "        self.explicit_bandwidth = bandwidth\n",
        "        self.bandwidth_heuristic = bandwidth_heuristic\n",
        "        self.norm_size = tuple(normalized_size) if type(normalized_size) is not int else (normalized_size,)\n",
        "        self.ndim = len(self.norm_size)\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_size)) if bias else None\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Apply Kernel Density Normalization to the input tensor\"\"\"\n",
        "        self.extra_size = x.shape[:-self.ndim]\n",
        "        slice_shape = [prod(self.extra_size), prod(self.norm_size)]\n",
        "        x = x.reshape(slice_shape)\n",
        "        x = torch.stack([\n",
        "            self.kde.normalize_data(slice, bandwidth=self.explicit_bandwidth, bandwidth_heuristic=self.bandwidth_heuristic) for slice in x\n",
        "        ])\n",
        "        x = x.reshape(self.extra_size + self.norm_size)\n",
        "        x = x * self.weight\n",
        "        if self.bias is not None:\n",
        "            x = x + self.bias\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jylpL083efiv"
      },
      "source": [
        "The cell below contains the model for you to train on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "feX2eU6jefiw"
      },
      "outputs": [],
      "source": [
        "'''class MLP_LN_LogReg(nn.Module): # Use this model for training\n",
        "    def __init__(self, input_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        ) # Affine layer w/ ReLU activation\n",
        "        \"---This is where we use our KDLayerNorm---\"\n",
        "        self.layer_norm = KDLayerNorm(hidden_dim) # Our KDLayerNorm\n",
        "        \"------------------------------------------\"\n",
        "        self.log_reg = nn.Linear(hidden_dim, 1)  # Logistic regression layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.log_reg(x)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, dim, eps = 1e-8):\n",
        "    super().__init__()\n",
        "    self.scale = nn.Parameter(torch.ones(dim))\n",
        "    self.eps = eps\n",
        "\n",
        "  def forward(self, x):\n",
        "    norm = x.norm(2, dim = -1, keepdim = True)\n",
        "    return x * self.scale / (norm + self.eps)"
      ],
      "metadata": {
        "id": "ZO2Clvt_esuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_LN_LogReg(nn.Module): # Use this model for training\n",
        "    def __init__(self, input_dim, hidden_dim = 256):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            RMSNorm(hidden_dim),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(0.02),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            RMSNorm(hidden_dim),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(0.02),\n",
        "\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            RMSNorm(hidden_dim // 2),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(0.02),\n",
        "        )\n",
        "\n",
        "        #self.layer_norm = KDLayerNorm(hidden_dim // 2)\n",
        "        self.log_reg = nn.Linear(hidden_dim // 2, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.mlp(x)\n",
        "       # x = self.layer_norm(x)\n",
        "        x = self.log_reg(x)\n",
        "        return torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "Ehz7F4AbI1q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqVs8Fo0efiw",
        "outputId": "9073c467-fd94-4f67-df7a-64440bba94af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Loss: 0.3859\n",
            "Epoch [20/50], Loss: 0.3024\n",
            "Epoch [30/50], Loss: 0.2686\n",
            "Epoch [40/50], Loss: 0.2507\n",
            "Epoch [50/50], Loss: 0.2429\n",
            "Test Accuracy: 99.12%\n"
          ]
        }
      ],
      "source": [
        "# The following imports may be useful\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# TODO: your code for model training\n",
        "\n",
        "#first step would be to load and preprocess the data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "#now lets normalize the featurs using standard scaler\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "#converting to pytorch tensors\n",
        "X = torch.tensor(X, dtype = torch.float32)\n",
        "y = torch.tensor(y, dtype = torch.float32).view(-1, 1) #Reshape y to match the output thought to do so from the error of hibiki shape mismatch I saw\n",
        "\n",
        "#splitting the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
        "\n",
        "#Next step is to initialize the model and training components\n",
        "input_dim = X_train.shape[1] #number of features\n",
        "#hidden_dim = 256 #size fo the hidden layer\n",
        "\n",
        "model = MLP_LN_LogReg(input_dim)\n",
        "criterion = nn.BCELoss() #Binary Cross entropy loss\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 0.001, weight_decay = 1e-5) #added weight decay for regulariztion\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.5)\n",
        "\n",
        "#third step is to train the model\n",
        "epochs = 50\n",
        "batch_size = 64\n",
        "num_batches = len(X_train) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  model.train()#setting the model to training mode\n",
        "  epoch_loss = 0.0\n",
        "\n",
        "  for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = min(start + batch_size, len(X_train))#this will handle the last batch properly\n",
        "    X_batch = X_train[start:end]\n",
        "    y_batch = y_train[start:end]\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = model(X_batch)\n",
        "    loss = criterion(y_pred, y_batch)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "  scheduler.step()\n",
        "\n",
        "\n",
        "  #average loss\n",
        "  avg_loss = epoch_loss / num_batches\n",
        "\n",
        "  #printing the loss after 10 epochs\n",
        "  if (epoch + 1) % 10 == 0:\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "#last step is to evaluate model\n",
        "with torch.no_grad():\n",
        "  y_test_pred = model(X_test)\n",
        "  y_test_pred = (y_test_pred > 0.5).float() #here converting the probablities to binary\n",
        "  accuracy = (y_test_pred == y_test).float().mean().item()\n",
        "\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0YWuFTuefiw"
      },
      "source": [
        "### Bonus exercise for extra points: Squeeze out all the juice!\n",
        "For extra points, edit the model architecture so that when you train it **with the same number of training iterations**, the model achieves a higher _test accuracy_ (higher _training accuracy_ alone does not count) For this task, you may edit the model directly within the cell we provided."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x1nXzsr4-IfN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}